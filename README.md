# Papers

## Bese Method
1.  Mixup: Mixup: Beyond Empirical Risk Minimization -- ICLR 2018 [Paper](https://arxiv.org/pdf/1710.09412) [Code](https://github.com/facebookresearch/mixup-cifar10) 
2.  Cutmix: 
2.  Framework: [Improved Mixed-Example Data Augmentation](https://arxiv.org/pdf/1805.11272.pdf?ref=https://githubhelp.com) [2018; CV]

## Theory
1.  [On Mixup Regularization](https://arxiv.org/pdf/2006.06049.pdf) [2020]
2.  [On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks](https://proceedings.neurips.cc/paper/2019/file/36ad8b5f42db492827016448975cc22d-Paper.pdf) [2019]
3.  [Understanding Mixup Training Methods](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478159) [2018]
4.  [Generalization Bounds for Vicinal Risk Minimization Principle](https://arxiv.org/pdf/1811.04351) [2018]
5.  [MixUp as Directional Adversarial Training](http://arxiv.org/pdf/1906.06875)[2019]
6.  [FMix: Enhancing Mixed Sample Data Augmentation](https://arxiv.org/pdf/2002.12047)[2020]

## Variants
1.  CutMix: [CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.pdf) [2019; CV, Robustness, Uncertainty]
2.  RICAP: [RICAP: Random Image Cropping and Patching Data Augmentation for Deep CNNs](http://proceedings.mlr.press/v95/takahashi18a/takahashi18a.pdf) [2018; CV]
3.  BC: [Learning from Between-Class Examples for Deep Sound Recognition](https://arxiv.org/pdf/1711.10282.pdf) [2018, Sound]
4.  Manifold Mixup: [Manifold Mixup: Better Representations by Interpolating Hidden States](http://proceedings.mlr.press/v97/verma19a/verma19a.pdf) [2019; CV]
5.  AdaMixUp: [MixUp as Locally Linear Out-of-Manifold Regularization](https://ojs.aaai.org/index.php/AAAI/article/download/4256/4134) [2019; CV]
6.  Spatial-mixup: [Understanding Mixup Training Methods](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478159) [2018; CV, GAN]
7.  PuzzleMix: [Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup](http://proceedings.mlr.press/v119/kim20b/kim20b.pdf) [2020; CV, Robustness]
8.  SmoothMix: [SmoothMix: a Simple Yet Effective Data Augmentation to Train Robust Classifiers](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w45/Lee_SmoothMix_A_Simple_Yet_Effective_Data_Augmentation_to_Train_Robust_CVPRW_2020_paper.pdf)(2020; CV, Robustness)
9. ResizeMix: [ResizeMix: Mixing Data with Preserved Object Information and True Labels](https://arxiv.org/pdf/2012.11101.pdf)(2020; CV)
10. AugMix: [AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty](https://openreview.net/pdf?id=S1gmrxHFvB)(2020; CV, Robustness)
11. StackMix: [StackMix: A complementary Mix algorithm](https://proceedings.mlr.press/v180/chen22b/chen22b.pdf)(2022; CV, Robustness)
12. FocusMix: [Where to cut and paste: Data regularization with selective features](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9289404&casa_token=ZOkscThNTpQAAAAA:AhalGFG_kjrXgaEZRzo5E3QN2mNC7gdnF1PtAd7MG0-rXbaHSS1JzZiM5wWv7hLR8plKxXy4F3U)(2020; CV)
13. Mixup Inference: [Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks](https://arxiv.org/pdf/1909.11515)(2020; CV, Robustness)
14. SSMix: [SSMix: Saliency-Based Span Mixup for Text Classification](https://arxiv.org/pdf/2106.08062)(2021; NLP)
15. DJMix: [DJMix: Unsupervised Task-agnostic Augmentation for Improving Robustness](https://openreview.net/pdf?id=0n3BaVlNsHI)(2021; CV)
16. StyleMix: [StyleMix: Separating Content and Style for Enhanced Data Augmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_StyleMix_Separating_Content_and_Style_for_Enhanced_Data_Augmentation_CVPR_2021_paper.pdf)(2021; CV)
17. Local Mixup: [Preventing Manifold Intrusion with Locality: Local Mixup](https://arxiv.org/pdf/2201.04368)(2022)
18. Remix: [Remix: Rebalanced Mixup](https://arxiv.org/pdf/2007.03943)(2020)
19. k-Mixup: [k-Mixup Regularization for Deep Learning via Optimal Transport](https://arxiv.org/pdf/2106.02933)(2021)
20. MixMo: [MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks](https://openaccess.thecvf.com/content/ICCV2021/papers/Rame_MixMo_Mixing_Multiple_Inputs_for_Multiple_Outputs_via_Deep_Subnetworks_ICCV_2021_paper.pdf)(2021)
21. SaliencyMix: [SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization](https://arxiv.org/pdf/2006.01791)(2021)
22. SnapMix: [Snapmix: Semantically proportional mixing for augmenting fine-grained data](https://ojs.aaai.org/index.php/AAAI/article/view/16255/16062)(2021)
23. Co-Mixup: [Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity](https://arxiv.org/pdf/2102.03065)(2021)
24. SuperMix: [SuperMix: Supervising the Mixing Data Augmentation](http://openaccess.thecvf.com/content/CVPR2021/papers/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.pdf)(2021)
25. PatchMix: [Evolving Image Compositions for Feature Representation Learning](https://arxiv.org/pdf/2106.09011)(2021)
26. SAMix: [Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup](https://arxiv.org/pdf/2111.15454)(2021)
27. mWh: [Mixup Without Hesitation](https://arxiv.org/pdf/2101.04342)(2021)
28. CAMixup: [Combining Ensembles and Data Augmentation can Harm your Calibration](https://arxiv.org/pdf/2010.09875)(2021)
29. GridMix: [GridMix: Strongregularizationthroughlocalcontextmapping](https://www.sciencedirect.com/science/article/pii/S0031320320303976?casa_token=oQ7NhHPxs1cAAAAA:U0cFG2ASbufAHEPW4m14bxaUMsXK3QE6ke-sjpvbpkcbbLAd_YFSUEbUU2DECq3H7IjtW2dRpAQ)(2021)
30. BATCHMIXUP: [BATCHMIXUP: Improving Training by Interpolating Hidden States of the Entire Mini-batch](https://aclanthology.org/2021.findings-acl.434.pdf)(2021)
31. AMP: [Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup](https://arxiv.org/pdf/2109.07177)(2021)
32. AutoMix: [AutoMix: Unveiling the Power of Mixup for Stronger Classifiers](https://arxiv.org/pdf/2103.13027)(2022)
33. PatchUp: [PatchUp: A Regularization Technique for Convolutional Neural Networks](PatchUp)(2022)
34. MultiMix: [Teach me how to Interpolate a Myriad of Embeddings](https://arxiv.org/pdf/2205.14230)(2022)
35. DM: [Decoupled Mixup for Data-efficient Learning](https://arxiv.org/pdf/2203.10761)(2022)
36. TransMix: [TransMix: Attend to Mix for Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_TransMix_Attend_To_Mix_for_Vision_Transformers_CVPR_2022_paper.pdf)(2022)
37. TokenMix: [TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers](https://arxiv.org/pdf/2207.08409)(2022)
38. Saliency Grafting: [Saliency Grafting: Innocuous Attribution-Guided Mixup with Calibrated Label Mixing](https://ojs.aaai.org/index.php/AAAI/article/view/20766/20525)(2022)
39. RecursiveMix: [RecursiveMix: Mixed Learning with History](https://arxiv.org/pdf/2203.06844)(2022)
40. ScoreMix: [ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification](https://arxiv.org/pdf/2202.07570)(2022)
41. PixMix: [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://openaccess.thecvf.com/content/CVPR2022/papers/Hendrycks_PixMix_Dreamlike_Pictures_Comprehensively_Improve_Safety_Measures_CVPR_2022_paper.pdf)(2022)
42. AlignMixup: [AlignMixup: Improving Representations By Interpolating Aligned Features](https://openaccess.thecvf.com/content/CVPR2022/papers/Venkataramanan_AlignMixup_Improving_Representations_by_Interpolating_Aligned_Features_CVPR_2022_paper.pdf)(2022)
43. Pani: [Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy](https://arxiv.org/pdf/1911.09307)(2019)
44. HypMix: [HYPMIX: Hyperbolic Interpolative Data Augmentation](https://aclanthology.org/2021.emnlp-main.776)(2021)
45. LADA: [Local Additivity Based Data Augmentation for Semi-supervised NER](https://arxiv.org/pdf/2010.01677)(2020)
46. Mixup-with-AUM-and-SM: [On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency](https://arxiv.org/pdf/2203.07559)(2022)
47. RandomMix: [RandomMix: A mixed sample data augmentation method with multiple mixed modes](https://arxiv.org/pdf/2205.08728)(2022)
48. SuperpixelGridMasks: [SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix Data Augmentation](https://arxiv.org/pdf/2204.08458)(2022)
49. M-mix: [M-Mix: Generating Hard Negatives via Multi-sample Mixing for Contrastive Learning](https://dl.acm.org/doi/pdf/10.1145/3534678.3539248)(2022)
50. DMix: [DMIX: Adaptive Distance-aware Interpolative Mixup](https://aclanthology.org/2022.acl-short.67)(2022)
51. CSANMT: [Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation](https://arxiv.org/pdf/2204.06812)(2022)
53. GMix: [A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective](https://arxiv.org/pdf/2208.09913)(2022)
54. DMixup & DCutmix: [Observations on K-image Expansion of Image-Mixing Augmentation for Classification](https://arxiv.org/pdf/2110.04248)(2021)

## Others
1.  SamplePairing: [Data Augmentation by Pairing Samples for Images Classification](https://arxiv.org/pdf/1801.02929) [2018; CV]
2.  Cutout: [Improved Regularization of Convolutional Neural Networks with Cutout](https://arxiv.org/pdf/1708.04552.pdf) [2018; CV]
3.  MixFeat: [Mixfeat: Mix feature in latent space learns discriminative space](https://openreview.net/pdf?id=HygT9oRqFX) [2019; CV]
4.  FMix: [FMix: Enhancing Mixed Sample Data Augmentation](https://arxiv.org/pdf/2002.12047)[2020; CV, Robustness]
5.  MoEx: [On feature normalization and data augmentation](http://openaccess.thecvf.com/content/CVPR2021/papers/Li_On_Feature_Normalization_and_Data_Augmentation_CVPR_2021_paper.pdf)[2021]

## Applications
### CV
1.  BC+: [Between-class Learning for Image Classification](https://openaccess.thecvf.com/content_cvpr_2018/papers/Tokozume_Between-Class_Learning_for_CVPR_2018_paper.pdf) (2018; CV)
### NLP
1.  WordMixup & SenMixup: [Augmenting Data with Mixup for Sentence Classification: An Empirical Study](https://arxiv.org/pdf/1905.08941.pdf) [2019; NLP]
2.  TMix & MixText: [MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification](https://arxiv.org/pdf/2004.12239.pdf) [2020; NLP]
3.  Mixup-Transformer: [Mixup-Transformer: Dynamic Data Augmentation for NLP Tasks](https://arxiv.org/pdf/2010.02394) [2020; NLP]
4.  SeqMix: [SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup](https://arxiv.org/pdf/2010.02322) [2020; NLP]
5.  Nonlinear Mixup: [Nonlinear Mixup: Out-Of-Manifold Data Augmentation for Text Classification](https://ojs.aaai.org/index.php/AAAI/article/view/5822/5678) [2020; NLP]
6.  SSMix: [SSMix: Saliency-Based Span Mixup for Text Classification](https://arxiv.org/pdf/2106.08062)(2021; NLP)
7.  SeqMix: [Sequence-Level Mixed Sample Data Augmentation](https://arxiv.org/pdf/2011.09039)(2020)
8.  AMDA: [Better Robustness by More Coverage: Adversarial and Mixup Data Augmentation for Robust Finetuning](https://aclanthology.org/2021.findings-acl.137.pdf)(2021)
9.  AdvAug: [AdvAug: Robust Adversarial Augmentation for Neural Machine Translation](https://arxiv.org/pdf/2006.11834)(2020)
10. Snippext: [Snippext: Semi-supervised Opinion Mining with Augmented Data](https://dl.acm.org/doi/pdf/10.1145/3366423.3380144)(2020)
11. Emix: [Augmenting NLP models using Latent Feature Interpolations](https://aclanthology.org/2020.coling-main.611)(2020)
12. Calibrated-BERT-Fine-Tuning: [Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data](http://arxiv.org/pdf/2010.11506)(2020)
13. STEMM: [STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation](https://arxiv.org/pdf/2203.10426)(2022)
14. X-Mixup: [Enhancing Cross-lingual Transfer by Manifold Mixup](https://arxiv.org/pdf/2205.04182)(2022)
15. mXEncDec: [Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation](https://arxiv.org/pdf/2203.07627)(2022)
### Semi-Supervised
1.  ICT: [Interpolation Consistency Training for Semi-Supervised Learning](https://arxiv.org/pdf/1903.03825.pdf?ref=https://githubhelp.com) [2019; CV]
2.  MixMatch: [MixMatch: A Holistic Approach to Semi-Supervised Learning](https://proceedings.neurips.cc/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf) [2019; CV]
3.  ReMixMatch: [ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring](https://arxiv.org/pdf/1911.09785.pdf)[2019].
4.  DivideMix: [DivideMix: Learning with Noisy Labels as Semi-supervised Learning](https://arxiv.org/pdf/2002.07394.pdf)(2020)
### Unsupervised: 
1.  ACAI: [Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer](https://arxiv.org/pdf/1807.07543.pdf) [2019; AE, GAN, CV]
2.  AMR: [On Adversarial Mixup Resynthesis](https://proceedings.neurips.cc/paper/2019/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf) [2019; AE, GAN, CV]
3.  LSI: [Data Augmentation via Latent Space Interpolation for Image Classification](https://ieeexplore.ieee.org/iel7/8527858/8545020/08545506.pdf)(2018)
### Domain Adaptation 
1.  VMT: [Virtual Mixup Training for Unsupervised Domain Adaptation](https://arxiv.org/pdf/1905.04215.pdf) [2019; CV]
2.  DMRL: [Dual Mixup Regularized Learning for Adversarial Domain Adaptation](https://arxiv.org/pdf/2007.03141) [2020; CV]
3.  IIMT: [Improve Unsupervised Domain Adaptation with Mixup Training](https://arxiv.org/pdf/2001.00677) [2022; CV]
4.  Mixstyle: [Domain generalization with mixstyle](https://arxiv.org/pdf/2104.02008)[2021]
### Robustness
1.  M-TLAT: [Addressing Neural Network Robustness with Mixup and Targeted Labeling Adversarial Training](https://arxiv.org/pdf/2008.08384) [2022; Robustness]
2. IAT: [Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy](https://dl.acm.org/doi/pdf/10.1145/3338501.3357369)
3. Mixup Inference: [Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks](https://arxiv.org/pdf/1909.11515)(2020; CV, Robustness)
4. MixACM: [MixACM: Mixup-Based Robustness Transfer via Distillation of Activated Channel Maps](https://proceedings.neurips.cc/paper/2021/file/240c945bb72980130446fc2b40fbb8e0-Paper.pdf)(2021)
5. AVmixup: [Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization](https://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Adversarial_Vertex_Mixup_Toward_Better_Adversarially_Robust_Generalization_CVPR_2020_paper.pdf)(2020)
6. AOM: [Adversarially Optimized Mixup for Robust Classification](https://arxiv.org/pdf/2103.11589)(2021)
7. VarMixup: [VarMixup: Exploiting the Latent Space for Robust Training and Inference](https://arxiv.org/pdf/2003.06566v1.pdf)(2020)
8. GIF: [Guided Interpolation for Adversarial Training](https://arxiv.org/pdf/2102.07327)
9. Mixup-SSAT: [Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction](https://arxiv.org/pdf/2205.14230)(2022)
10. MixDiversity: [Mixup Decoding for Diverse Machine Translation](https://arxiv.org/pdf/2109.03402)(2021)

### Federated Learning
1. Mix2FLD: [Mix2FLD: Downlink Federated Learning After Uplink Federated Distillation With Two-Way Mixup](https://ieeexplore.ieee.org/iel7/4234/5534602/09121290.pdf)(2020)
2. XOR Mixup: [XOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning](https://arxiv.org/pdf/2006.05148)(2020)
3. FedMix: [FedMix: Approximation of Mixup under Mean Augmented Federated Learning](http://arxiv.org/pdf/2107.00233)(2021)
### Graph
1. GraphMix: [GraphMix: Improved Training of GNNs for Semi-Supervised Learning](https://ojs.aaai.org/index.php/AAAI/article/view/17203/17010)(2021)
### Point Cloud
1. PointMixup: [PointMixup: Augmentation for Point Clouds](https://arxiv.org/pdf/2008.06374)(2020)
2. RSMix: [Regularization Strategy for Point Cloud via Rigidly Mixed Sample](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Regularization_Strategy_for_Point_Cloud_via_Rigidly_Mixed_Sample_CVPR_2021_paper.pdf)(2021)
### Super-resolution
1. CutBlur: [Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy](http://openaccess.thecvf.com/content_CVPR_2020/papers/Yoo_Rethinking_Data_Augmentation_for_Image_Super-resolution_A_Comprehensive_Analysis_and_CVPR_2020_paper.pdf)
### Contrastive Learning
1. Mochi: [Hard Negative Mixing for Contrastive Learning](https://proceedings.neurips.cc/paper/2020/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf)(2020)
2. BSIM: [Beyond Single Instance Multi-view Unsupervised Representation Learning](https://arxiv.org/pdf/2011.13356)(2020)
3. Un-Mix: [Un-mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning](https://ojs.aaai.org/index.php/AAAI/article/view/20119/19878)(2022)
4. MixCo: [MixCo: Mix-up Contrastive Learning for Visual Representation](https://arxiv.org/pdf/2010.06300)(2020)
5. i-Mix: [i-mix: A domain-agnostic strategy for contrastive representation learning](https://arxiv.org/pdf/2010.08887)(2020)
5. MixSiam: [MixSiam: A Mixture-based Approach to Self-supervised Representation Learning](https://arxiv.org/pdf/2111.02679)(2021)
6. Core-tuning: [Unleashing the Power of Contrastive Self-Supervised Visual Models via Contrast-Regularized Fine-Tuning](https://proceedings.neurips.cc/paper/2021/file/fa14d4fe2f19414de3ebd9f63d5c0169-Paper.pdf)(2021)
7. Feature-Transformation: [Improving Contrastive Learning by Visualizing Feature Transformation](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Improving_Contrastive_Learning_by_Visualizing_Feature_Transformation_ICCV_2021_paper.pdf)(2021)
8. ProGCL: [ProGCL: Rethinking Hard Negative Mining in Graph Contrastive Learning](https://proceedings.mlr.press/v162/xia22b/xia22b.pdf)(2022)
### Metric Learning
1. Embedding expansion: [Embedding expansion: Augmentation in embedding space for deep metric learning](http://openaccess.thecvf.com/content_CVPR_2020/papers/Ko_Embedding_Expansion_Augmentation_in_Embedding_Space_for_Deep_Metric_Learning_CVPR_2020_paper.pdf)(2020)
2. Metrix: [It Takes Two to Tango: Mixup for Deep Metric Learning](https://arxiv.org/pdf/2106.04990.pdf)(2022)
### Others
1. C2L: [Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs By Comparing Image Representations](https://arxiv.org/pdf/2007.07423)
2. Mixup: [Mixup Learning Strategies for Text-independent Speaker Verification](https://cse.hkust.edu.hk/~mak/PDF/is2019-mixup.pdf)(2019)
3. Mixup: [An investigation of mixup training strategies for acoustic models in ASR](https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/2191.pdf)(2018)
4. VLMixer: [VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix](https://proceedings.mlr.press/v162/wang22h/wang22h.pdf)(2022)
5. VideoMix: [VideoMix: Rethinking Data Augmentation for Video Classification](https://arxiv.org/pdf/2012.03457)(2020)

# Code Repositories
{_Method_}\* denotes the code of _Method_ is reimplemented by others.
1.  Mixip: https://github.com/facebookresearch/mixup-cifar10
2.  Framework: https://github.com/ceciliaresearch/MixedExample
3.  CutMix: https://github.com/clovaai/CutMix-PyTorch
4.  RICAP*: https://github.com/4uiiurz1/pytorch-ricap
5.  BC: https://github.com/mil-tokyo/bc_learning_sound
6.  BC+: https://github.com/mil-tokyo/bc_learning_image
7.  Manifold Mixup: https://github.com/vikasverma1077/manifold_mixup
8.  ICT: https://github.com/vikasverma1077/ICT
9.  AdaMixUp*: https://github.com/SITE5039/AdaMixUp
10.  Cutout: https://github.com/uoguelph-mlrg/Cutout
11.  AMR: https://github.com/christopher-beckham/amr
12.  MixMatch: https://github.com/google-research/mixmatch
13.  ACAI: https://github.com/brain-research/acai
14.  VMT: https://github.com/xudonmao/VMT
15.  DMRL: https://github.com/YuanWu3/Dual-Mixup-Regularized-Learning-for-Adversarial-Domain-Adaptation
16.  TMix & MixText: https://github.com/GT-SALT/MixText
17.  Spatial-mixup: https://github.com/liangdaojun/spatial-mixup/tree/master/models
18.  SeqMix: https://github.com/rz-zhang/SeqMix
19.  PuzzleMix: https://github.com/snu-mllab/PuzzleMix
20.  FMix: https://github.com/ecs-vlc/FMix
21.  SmoothMix: https://github.com/jh-jeong/smoothmix
22.  ResizeMix*: https://github.com/HarborYuan/pytorch-macos-bench
23.  AugMix: https://github.com/google-research/augmi
24.  Mixup Inference: https://github.com/P2333/Mixup-Inference
25.  FedMix: https://github.com/smduan
26.  SSMix: https://github.com/clovaai/ssmix
27.  GraphMix: https://github.com/vikasverma1077/GraphMix
28.  PointMixup: https://github.com/yunlu-chen/PointMixup
29.  PA-AUG: https://github.com/sky77764/pa-aug.pytorch
30.  RSMix: https://github.com/dogyoonlee/RSMix
31.  ReMixMatch: https://github.com/google-research/remixmatch
32.  Cutblur: https://github.com/clovaai/cutblur
33.  StyleMix: https://github.com/alsdml/StyleMix
34.  Mixmo: https://github.com/alexrame/mixmo-pytorch
35.  MoEx: https://github.com/Boyiliee/MoEx
36.  Mixstyle: https://github.com/KaiyangZhou/mixstyle-release
37.  SaliencyMix: https://github.com/afm-shahab-uddin/SaliencyMix
38.  SnapMix: https://github.com/Shaoli-Huang/SnapMix.git
39.  Co-Mixup: https://github.com/snu-mllab/Co-Mixup
40.  SuperMix: https://github.com/alldbi/SuperMix
41.  Mochi: https://europe.naverlabs.com/mochi
42.  Embedding-expansion: https://github.com/clovaai/embedding-expansion
43.  C2L: https://github.com/funnyzhou/C2L_MICCAI2020
44.  Un-Mix: https://github.com/szq0214/Un-Mix
45.  MixCo: https://github.com/Lee-Gihun/MixCo-Mixup-Contrast
46.  mwh: https://github.com/yuhao318/mwh
47.  DivideMix: https://github.com/LiJunnan1992/DivideMix
48.  CAMixup: https://github.com/google/edward2/tree/master/experimental/marginalization_mixup
49.  DivideMix: https://github.com/LiJunnan1992/DivideMix
50.  MixACM: https://awaisrauf.github.io/MixACM
51.  AVmixup: https://github.com/xuyinhu/AVmixup
52.  AMP: https://github.com/PAI-SmallIsAllYourNeed/Mixup-AMP
53.  i-Mix: https://github.com/kibok90/imix
54.  AutoMix: https://github.com/Westlake-AI/openmixup
55.  PatchUp: https://github.com/chandar-lab/PatchUp
56.  Metrix: https://tinyurl.com/metrix-iclr
57.  TransMix: https://github.com/Beckschen/TransMix
58.  TokenMix: https://github.com/Sense-X/TokenMix
59.  RecursiveMix: https://github.com/megvii-research/RecursiveMix
60.  Pixmix: https://github.com/andyzoujm/pixmix
61.  AlignMixup: https://github.com/shashankvkt/AlignMixup_CVPR22.git
62.  AMDA: https://github.com/thunlp/MixADA
63.  HypMix: https://github.com/caisa-lab/hypmix-emnlp
64.  LADA: https://github.com/GT-SALT/LADA
65.  Snippext: https://github.com/rit-git/Snippext_public
66.  Core-tuning: https://github.com/Vanint/Core-tuning
67.  Feature-Transformation: https://github.com/DTennant/CL-Visualizing-Feature-Transformation
68.  Mixup-with-AUM-and-SM: https://github.com/seoyeon-p/MixUp-Guided-by-AUM-and-Saliency-Map
69.  Calibrated-BERT-Fine-Tuning: https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning
70.  ProGCL: https://github.com/junxia97/ProGCL
71.  STEMM: https://github.com/ictnlp/STEMM
72.  X-Mixup: https://github.com/yhy1117/X-Mixup
73.  SuperpixelGridMasks: https://github.com/hammoudiproject/SuperpixelGridMasks
74.  M-mix: https://github.com/Sherrylone/m-mix
75.  DMix: https://github.com/caisa-lab/DMix-ACL
76.  VLMixer: https://github.com/ttengwang/VLMixer
77.  CSANMT: https://github.com/pemywei/csanmt
78.  GMix: https://github.com/naver-ai/hmix-gmix

## Keys
1.  Number of samples for mixup == 2;
2.  Data Augment v.s. Regularization --> Mixup v.s. Weight Decay;
3.  Why and which specific properties;
4.  Limits;
    in some way distorted, i.e., vicinal distribution not match the true distribution of the data
5.  Order of data augmentation;
6.  the role of \alpha;